{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook focuses on the second part of the PatientOmics framework:\n",
    "\n",
    "The aim is to organizing clinical time-series data for analysis and visualization, inwhich we perform the following tasks:\n",
    "\n",
    "1. Execute a script to generate the initial matrix containing clinical features for 116 patients, specifically on the first day of hospitalization.\n",
    "\n",
    "2. Visualize the correlation among laboratory features for potential long COVID patients (N = 116) on their first day of hospitalization, by:\n",
    "        a. Grouping features based on their laboratory families.\n",
    "        b. Sorting features by their average similarity distances.\n",
    "\n",
    "3. Create a cluster map to visualize data patterns among potential long COVID patients (N = 116).\n",
    "\n",
    "4. Structuring this notebook to add the data preparation functions to transform a pandas dataframe into X and y numpy arrays that can be used to create a TSDataset.\n",
    "    - https://timeseriesai.github.io/tsai/data.preparation.html\n",
    "\n",
    "\n",
    "\n",
    "https://github.com/tslearn-team/tslearn\n",
    "\n",
    "https://github.com/timeseriesAI/tsai\n",
    "\n",
    "https://github.com/timeseriesAI/tsai/blob/main/tutorial_nbs/15_PatchTST_a_new_transformer_for_LTSF.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the clinical features for all patients at one time point (e.g. 1st day of hospitalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get the clinical features for all patients at one time point (e.g. 1st day of hospitalization)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Read the list of unique patient IDs\n",
    "dir_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/05_data_exploration/01_preprocessing_116_PLCP/'\n",
    "filename = os.path.join(dir_path, '116_plcp_mv_pseudoid_pid.csv')\n",
    "df = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "# Initialize an empty list to store patient DataFrames\n",
    "patient_dfs = []\n",
    "\n",
    "## Set the day to be selected\n",
    "days_col = 7  \n",
    "\n",
    "# Iterate through the patient IDs\n",
    "for patient_id in df['pseudoid_pid']:\n",
    "    dir_lab_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/06_clinical_data/lab_data_features/'\n",
    "    filename = os.path.join(dir_lab_path, f'patient_{patient_id}.csv')\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame for each patient\n",
    "        df_lab = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "        # Get the row features for a specific day in 'days' column (e.g., 'days' == 0)\n",
    "        df_lab_selected_day = df_lab[df_lab['days'] == days_col].copy()\n",
    "\n",
    "        # Add a column 'patient_id' into the df_lab_selected_day\n",
    "        df_lab_selected_day['patient_id'] = patient_id\n",
    "\n",
    "        # Append the patient DataFrame to the list\n",
    "        patient_dfs.append(df_lab_selected_day)\n",
    "\n",
    "    except Exception as e:\n",
    "    # except FileNotFoundError:\n",
    "        print(f\"Error reading patient {patient_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all patient DataFrames into one\n",
    "collection_df = pd.concat(patient_dfs, ignore_index=True)\n",
    "\n",
    "# Save the resulting DataFrame as a CSV file\n",
    "output_csv_path = dir_path + '116_plcp_lab_markers_day-' + str(days_col) +'.csv'\n",
    "collection_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline method for data imputation for missing values from a range of days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def impute_missing_values(df_lab_selected_day, df_lab_days, columns_to_impute):\n",
    "    \"\"\"Function for data imputation using substitution method\"\"\"\n",
    "\n",
    "    imputed_values = []  # Store imputed values\n",
    "    \n",
    "    for column in columns_to_impute:\n",
    "        if not df_lab_selected_day[column].isna().all():\n",
    "            for index, row in df_lab_selected_day.iterrows():\n",
    "                if pd.isna(row[column]):\n",
    "                    # Find the nearest available value from days after the missing day\n",
    "                    next_day_values = df_lab_days[df_lab_days['days'] > row['days']].sort_values(by='days')\n",
    "                    if not next_day_values.empty:\n",
    "                        next_day_value = next_day_values.iloc[0][column]\n",
    "                        df_lab_selected_day.at[index, column] = next_day_value\n",
    "                        imputed_values.append(next_day_value)\n",
    "                    else:\n",
    "                        # If no values are available from days after, use the nearest available value from days before\n",
    "                        prev_day_values = df_lab_days[df_lab_days['days'] < row['days']].sort_values(by='days', ascending=False)\n",
    "                        if not prev_day_values.empty:\n",
    "                            prev_day_value = prev_day_values.iloc[0][column]\n",
    "                            df_lab_selected_day.at[index, column] = prev_day_value\n",
    "                            imputed_values.append(prev_day_value)\n",
    "    \n",
    "    return df_lab_selected_day, imputed_values\n",
    "\n",
    "# Read the list of unique patient IDs\n",
    "dir_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/05_data_exploration/01_preprocessing_116_PLCP/'\n",
    "filename = os.path.join(dir_path, '116_plcp_mv_pseudoid_pid.csv')\n",
    "df = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "# Initialize an empty list to store patient DataFrames\n",
    "patient_dfs = []\n",
    "\n",
    "# Set the day range to be selected\n",
    "init_day = 0\n",
    "end_day = 20\n",
    "\n",
    "# Initialize a list to store all imputed values\n",
    "all_imputed_values = []\n",
    "\n",
    "# Iterate through the patient IDs\n",
    "for patient_id in df['pseudoid_pid']:\n",
    "    dir_lab_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/06_clinical_data/lab_data_features/'\n",
    "    filename = os.path.join(dir_lab_path, f'patient_{patient_id}.csv')\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame for each patient\n",
    "        df_lab = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "        # Get the row features from 'df_lab' for the selected day range\n",
    "        df_lab_days = df_lab[df_lab['days'].between(init_day, end_day)].copy()\n",
    "\n",
    "        # Get the columns to impute from the dataframe df_lab_days\n",
    "        columns_to_impute = df_lab_days.columns[2:-1].tolist()\n",
    "\n",
    "        # Iterate through the selected days\n",
    "        for selected_day in range(init_day, end_day + 1):\n",
    "            # Get the row features for a specific day in 'days' column\n",
    "            df_lab_day_selected = df_lab_days[df_lab_days['days'] == selected_day].copy()\n",
    "\n",
    "            # Apply data imputation for the selected day\n",
    "            df_lab_day_selected, imputed_values = impute_missing_values(df_lab_day_selected, df_lab_days, columns_to_impute)\n",
    "            all_imputed_values.extend(imputed_values)  # Append imputed values to the list\n",
    "\n",
    "            # Add a column 'patient_id' into the df_lab_day_selected\n",
    "            df_lab_day_selected['patient_id'] = patient_id\n",
    "\n",
    "            # Append the patient DataFrame to the list\n",
    "            patient_dfs.append(df_lab_day_selected)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading patient {patient_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all patient DataFrames into one\n",
    "collection_df = pd.concat(patient_dfs, ignore_index=True)\n",
    "\n",
    "# Save the resulting DataFrame as a CSV file\n",
    "output_csv_path = os.path.join(dir_path, f'116_plcp_lab_markers_day_Imputed-{init_day}_to_{end_day}.csv')\n",
    "collection_df.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Save the imputed values to a separate file if needed\n",
    "imputed_values_file = os.path.join(dir_path, f'116_plcp_lab_imputed_values.csv')\n",
    "pd.Series(all_imputed_values).to_csv(imputed_values_file, index=False, header=['Imputed_Values'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#######################################################################################################################\n",
    "######################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Script for creating TSDataset based on hospitalization days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def impute_missing_values(df_lab_selected_day, df_lab_days, columns_to_impute):\n",
    "    \"\"\"Function for data imputation using substitution method\"\"\"\n",
    "    \n",
    "    imputed_values = []  # Store imputed values\n",
    "    \n",
    "    for column in columns_to_impute:\n",
    "        if not df_lab_selected_day[column].isna().all():\n",
    "            for index, row in df_lab_selected_day.iterrows():\n",
    "                if pd.isna(row[column]):\n",
    "                    # Find the nearest available value from days after the missing day\n",
    "                    next_day_values = df_lab_days[df_lab_days['days'] > row['days']].sort_values(by='days')\n",
    "                    if not next_day_values.empty:\n",
    "                        next_day_value = next_day_values.iloc[0][column]\n",
    "                        df_lab_selected_day.at[index, column] = next_day_value\n",
    "                        imputed_values.append(next_day_value)\n",
    "                    else:\n",
    "                        # If no values are available from days after, use the nearest available value from days before\n",
    "                        prev_day_values = df_lab_days[df_lab_days['days'] < row['days']].sort_values(by='days', ascending=False)\n",
    "                        if not prev_day_values.empty:\n",
    "                            prev_day_value = prev_day_values.iloc[0][column]\n",
    "                            df_lab_selected_day.at[index, column] = prev_day_value\n",
    "                            imputed_values.append(prev_day_value)\n",
    "    \n",
    "    return df_lab_selected_day, imputed_values\n",
    "\n",
    "# Read the list of unique patient IDs\n",
    "dir_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/05_data_exploration/01_preprocessing_116_PLCP/03_dataset/'\n",
    "# filename = os.path.join(dir_path, 'longcovid_patients_pseudoid_pid-train.csv')\n",
    "# filename = os.path.join(dir_path, 'deceased_patients_pseudoid_pid-train.csv')\n",
    "\n",
    "# filename = os.path.join(dir_path, 'longcovid_patients_pseudoid_pid-valid.csv')\n",
    "# filename = os.path.join(dir_path, 'deceased_patients_pseudoid_pid-valid.csv')\n",
    "\n",
    "# filename = os.path.join(dir_path, 'longcovid_patients_pseudoid_pid-test.csv')\n",
    "filename = os.path.join(dir_path, 'deceased_patients_pseudoid_pid-test.csv')\n",
    "\n",
    "\n",
    "df = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "# Initialize an empty list to store patient DataFrames\n",
    "patient_dfs = []\n",
    "\n",
    "# Set the day range to be selected\n",
    "init_day = 0\n",
    "end_day = 15\n",
    "\n",
    "# Initialize a list to store all imputed values\n",
    "all_imputed_values = []\n",
    "\n",
    "# Iterate through the patient IDs\n",
    "for patient_id in df['pseudoid_pid']:\n",
    "    dir_lab_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/06_clinical_data/lab_data_features/'\n",
    "    filename = os.path.join(dir_lab_path, f'patient_{patient_id}.csv')\n",
    "\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame for each patient\n",
    "        df_lab = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "        # Get the row features from 'df_lab' for the selected day range\n",
    "        df_lab_days = df_lab[df_lab['days'].between(init_day, end_day)].copy()\n",
    "\n",
    "        # Get the columns to impute from the dataframe df_lab_days\n",
    "        columns_to_impute = df_lab_days.columns[2:-1].tolist()\n",
    "\n",
    "        # Iterate through the selected days\n",
    "        for selected_day in range(init_day, end_day + 1):\n",
    "            # Get the row features for a specific day in 'days' column\n",
    "            df_lab_day_selected = df_lab_days[df_lab_days['days'] == selected_day].copy()\n",
    "\n",
    "            if df_lab_day_selected.empty:\n",
    "                # Create an empty row with the selected_day and patient_id\n",
    "                empty_data = {'days': [selected_day], 'patient_id': [patient_id]}\n",
    "                for col in columns_to_impute:\n",
    "                    empty_data[col] = [None]\n",
    "                df_lab_day_selected = pd.DataFrame(empty_data)\n",
    "\n",
    "            # Apply data imputation for the selected day\n",
    "            df_lab_day_selected, imputed_values = impute_missing_values(df_lab_day_selected, df_lab_days, columns_to_impute)\n",
    "            all_imputed_values.extend(imputed_values)  # Append imputed values to the list\n",
    "\n",
    "            # Add a column 'patient_id' into the df_lab_day_selected\n",
    "            df_lab_day_selected['patient_id'] = patient_id\n",
    "\n",
    "            # Append the patient DataFrame to the list\n",
    "            patient_dfs.append(df_lab_day_selected)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading patient {patient_id}: {str(e)}\")\n",
    "        continue\n",
    "\n",
    "# Concatenate all patient DataFrames into one\n",
    "collection_df = pd.concat(patient_dfs, ignore_index=True)\n",
    "\n",
    "# Save the resulting DataFrame as a CSV file\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_plcp_train-{init_day}_to_{end_day}.csv')\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_dcp_train-{init_day}_to_{end_day}.csv')\n",
    "\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_plcp_valid-{init_day}_to_{end_day}.csv')\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_dcp_valid-{init_day}_to_{end_day}.csv')\n",
    "\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_plcp_test-{init_day}_to_{end_day}.csv')\n",
    "output_csv_path = os.path.join(dir_path, f'tsdataset_dcp_test-{init_day}_to_{end_day}.csv')\n",
    "collection_df.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the multivariate-time-point matrix and group lab features by their laboratory family\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   patient_id        lab_parameter  \\\n",
      "0     1000766                 ALAT   \n",
      "1     1000766                 ASAT   \n",
      "2     1000766            Basophile   \n",
      "3     1000766      Bicarbonat Std.   \n",
      "4     1000766      Bicarbonat akt.   \n",
      "5     1000766  C-reaktives Protein   \n",
      "6     1000766          Eosinophile   \n",
      "7     1000766         Erythrozyten   \n",
      "8     1000766    Fibrinogen Clauss   \n",
      "9     1000766            Harnstoff   \n",
      "\n",
      "                                           lab_value  \n",
      "0  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "1  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "2  [0.01, 0.01, nan, 0.02, nan, 0.02, 0.03, nan, ...  \n",
      "3  [nan, 21.8, nan, nan, nan, nan, 17.6, nan, 20....  \n",
      "4  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "5  [88.0, 98.0, nan, 102.0, nan, 149.0, 103.0, na...  \n",
      "6  [0.01, 0.01, nan, 0.02, nan, 0.05, 0.04, nan, ...  \n",
      "7  [3.38, 3.26, nan, 3.24, nan, 3.24, 3.03, nan, ...  \n",
      "8  [nan, nan, nan, nan, nan, nan, nan, nan, nan, ...  \n",
      "9  [nan, 24.5, nan, nan, nan, 24.2, nan, nan, nan...  \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "## Load the list of laboratory features\n",
    "dir_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/05_data_exploration/01_preprocessing_116_PLCP/03_dataset/'\n",
    "filename = os.path.join(dir_path, '00_lab_parameter_grouping.csv')\n",
    "df_lab_features = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "\n",
    "## Read the list of unique patient IDs\n",
    "dir_path = '/home/jagh/Documents/01_UB/MultiOmiX/patientomics/data/05_data_exploration/01_preprocessing_116_PLCP/03_dataset/'\n",
    "# filename = os.path.join(dir_path, 'tsdataset_plcp_train-0_to_15.csv')\n",
    "# filename = os.path.join(dir_path, 'tsdataset_dcp_train-0_to_15.csv')\n",
    "\n",
    "# filename = os.path.join(dir_path, 'tsdataset_plcp_valid-0_to_15.csv')\n",
    "# filename = os.path.join(dir_path, 'tsdataset_dcp_valid-0_to_15.csv')\n",
    "\n",
    "# filename = os.path.join(dir_path, 'tsdataset_plcp_test-0_to_15.csv')\n",
    "filename = os.path.join(dir_path, 'tsdataset_dcp_test-0_to_15.csv')\n",
    "\n",
    "df = pd.read_csv(filename, sep=',', header=0)\n",
    "\n",
    "\n",
    "## Set the 'df' columns in the order of the 'df_lab_features'\n",
    "df = df.reindex(columns=df_lab_features['lab_parameter'].tolist())\n",
    "# print(df.head(10))\n",
    "\n",
    "\n",
    "################################################\n",
    "### Transform the 'df' into a time-series dataset\n",
    "df_copy = df.copy() \n",
    "\n",
    "## Transpose the 'df[:, 2:-1]' to get the laboratory features as rows per patient and set the 'patient_id' column as row index\n",
    "df_transposed = df_copy.melt(id_vars=['patient_id'], value_vars=df_copy.columns[2:-1], var_name='lab_parameter', value_name='lab_value')\n",
    "\n",
    "## Set all 'lab_value' of the same lab_parameter in on row per patient in a sparse dataframe\n",
    "df_transposed = df_transposed.groupby(['patient_id', 'lab_parameter'])['lab_value'].apply(list).reset_index()\n",
    "print(df_transposed.head(10))\n",
    "\n",
    "## save the resulting DataFrame as a CSV file\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_plcp_train-0_to_15_ts.csv')\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_dcp_train-0_to_15_ts.csv')\n",
    "\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_plcp_valid-0_to_15_ts.csv')\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_dcp_valid-0_to_15_ts.csv')\n",
    "\n",
    "\n",
    "# output_csv_path = os.path.join(dir_path, f'tsdataset_plcp_test-0_to_15_ts.csv')\n",
    "output_csv_path = os.path.join(dir_path, f'tsdataset_dcp_test-0_to_15_ts.csv')\n",
    "\n",
    "df_transposed.to_csv(output_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patientomics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
